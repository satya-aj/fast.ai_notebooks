
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/04_fastai_lesson12.ipynb

from nb_03 import *


class NoneReduce():
    def __init__(self, loss_func):
        self.loss_func, self.old_red = loss_func, None

    def __enter__(self):
        if hasattr(self.loss_func, 'reduction'):
            self.old_red = getattr(self.loss_func, 'reduction')
            setattr(self.loss_func, 'reduction', 'none')
            return self.loss_func
        else:
            return partial(self.loss_func, reduction='none')

    def __exit__(self, type, value, traceback):
        if self.old_red is not None:
            setattr(self.loss_func, 'reduction', self.old_red)

from torch.distributions.beta import Beta

def unsqueeze(input, dims):
    for dim in listify(dims):
        input = torch.unsqueeze(input, dim)
    return input

def reduce_loss(loss, reduction='mean'):
    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss


class Mixup(Callback):
    _order = 90
    def __init__(self, α:float=0.4):
        self.distrib = Beta(tensor([α]), tensor([α]))

    def begin_fit(self):
        self.old_loss_func, self.run.loss_func = self.run.loss_func, self.loss_func

    def begin_batch(self):
        if not self.in_train:
            return
        λ = self.distrib.sample((self.yb.size(0), )).squeeze().to(self.xb.device)
        λ = torch.stack([λ, 1-λ], 1)
        self.λ = unsqueeze(λ.max(1)[0], (1, 2, 3))
        shuffle = torch.randperm(self.yb.size(0)).to(self.xb.device)
        xb1, self.yb1 = self.xb[shuffle], self.yb[shuffle]
        self.run.xb = lin_comb(self.xb, xb1, self.λ)

    def after_fit(self):
        self.run.loss_func = self.old_loss_func

    def loss_func(self, pred, yb):
        if not self.in_train:
            return self.old_loss_func(pred, yb)
        with NoneReduce(self.old_loss_func) as loss_func:
            loss1 = loss_func(pred, yb)
            loss2 = loss_func(pred, self.yb1)
        loss = lin_comb(loss1, loss2, self.λ)
        return reduce_loss(loss, getattr(self.old_loss_func, 'reduction', 'mean'))

def lin_comb(v1, v2, beta):
    return beta * v1 + (1-beta) * v2

class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, ε:float=0.1, reduction='mean'):
        super().__init__()
        self.ε, self.reduction = ε, reduction

    def forward(self, output, target):
        c = output.size()[-1]
        log_preds = F.log_softmax(output, dim=-1)
        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)
        nll = F.nll_loss(log_preds, target, reduction=self.reduction)
        return lin_comb(loss/c, nll, self.ε)

import apex.fp16_utils as fp16

def get_master(opt, flat_master=False):
    model_pgs = [[param for param in pg if param.requires_grad] for pg in opt.param_groups]
    if flat_master:
        master_pgs = []
        for pg in model_pgs:
            mp = parameters_to_vector([param.data.float() for param in pg])
            mp = torch.nn.Parameter(mp, requires_grad=True)
            if mp.grad is None:
                mp.grad = mp.new(*mp.size())
            master_pgs.append([mp])
    else:
        master_pgs = [[param.clone().float().detach() for param in pg] for pg in model_pgs]
        for pg in master_pgs:
            for param in pg:
                param.requires_grad_(True)
        return model_pgs, master_pgs

def to_master_grads(model_pgs, master_pgs, flat_master:bool=False)->None:
    for (model_params, master_params) in zip(model_pgs, master_pgs):
        fp16.model_grads_to_master_grads(model_params, master_params, flat_master=flat_master)


def to_model_params(model_pgs, master_pgs, flat_master:bool=False)->None:
    for (model_params, master_params) in zip(model_pgs, master_pgs):
        fp16.master_params_to_model_params(model_params, master_params, flat_master=flat_master)

def test_overflow(x):
    s = float(x.float().sum())
    return (s == float("inf") or s == float("-inf") or s != s)

def grad_overflow(param_groups):
    for group in param_groups:
        for p in group:
            if p.grad is not None:
                s = float(p.grad.data.float().sum())
                if s == float("inf") or s == float("-inf") or s != s:
                    return True
    return False

class MixedPrecision(Callback):
    _order = 99
    def __init__(self, loss_scale=512, flat_master=False, dynamic=True, max_loss_scale=2.**24, div_factor=2., scale_wait=5000):
        assert torch.backends.cudnn.enabled, "Mixed precision training requires cudnn"
        self.flat_master, self.dynamic, self.max_loss_scale = flat_master, dynamic, max_loss_scale
        self.div_factor, self.scale_wait = div_factor, scale_wait
        self.loss_scale = max_loss_scale if dynamic else loss_scale

    def begin_fit(self):
        self.run.model = fp16.convert_network(self.model, dtype=torch.float16)
        self.model_pgs, self.master_pgs = get_master(self.opt, self.flat_master)
        self.run.opt.param_groups = self.master_pgs
        if self.dynamic:
            self.count = 0

    def begin_batch(self):
        self.run.xb = self.run.xb.half()

    def after_pred(self):
        self.run.pred = self.run.pred.float()

    def after_loss(self):
        if self.in_train:
            self.run.loss *= self.loss_scale

    def after_backward(self):
        if self.dynamic and grad_overflow(self.model_pgs):
            self.loss_scale /= self.div_factor
            self.model.zero_grad()
            return True
        to_master_grads(self.model_pgs, self.master_pgs, self.flat_master)
        for master_params in self.master_pgs:
            for param in master_params:
                if param.grad is not None:
                    param.grad.div_(self.loss_scale)
        if self.dynamic:
            self.count += 1
            if self.count == self.scale_wait:
                self.count = 0
                self.loss_scale *= self.div_factor

    def after_step(self):
        self.model.zero_grad()
        to_model_params(self.model_pgs, self.master_pgs, self.flat_master)

def noop(x):
    return x

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

def conv(ni, nf, ks=3, stride=1, bias=False):
    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)

act_fn = nn.ReLU(inplace=True)

def init_cnn(m):
    if getattr(m, "bias", None) is not None:
        nn.init.constant_(m.bias, 0)
    if isinstance(m, (nn.Conv2d, nn.Linear)):
        nn.init.kaiming_normal_(m.weight)

    for l in m.children():
        init_cnn(l)

def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):
    bn = nn.BatchNorm2d(nf)
    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)
    layers = [conv(ni, nf, ks, stride=stride), bn]
    if act:
        layers.append(act_fn)
    return nn.Sequential(*layers)

class ResBlock(nn.Module):
    def __init__(self, expansion, ni, nh, stride=1):
        super().__init__()
        nf, ni = nh * expansion, ni * expansion
        layers = [conv_layer(ni, nh, 3, stride=stride),
                    conv_layer(nh, nf, 3, zero_bn=True, act=False)
        ] if expansion == 1 else [conv_layer(ni, nh, 1),
                                  conv_layer(nh, nh, 3, stride=stride),
                                  conv_layer(nh, nf, 1, zero_bn=True, act=False)
        ]
        self.convs = nn.Sequential(*layers)
        self.idconv = noop if ni == nf else conv_layer(ni, nf, 1, act=False)
        self.pool = noop if stride == 1 else nn.AvgPool2d(2, ceil_mode=True)

    def forward(self, x):
        return act_fn(self.convs(x) + self.idconv(self.pool(x)))


class XResNet(nn.Sequential):
    @classmethod
    def create(cls, expansion, layers, c_in=3, c_out=1000):
        nfs = [c_in, (c_in + 1)*8, 64, 64]
        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i == 0 else 1) for i in range(3)]
        nfs = [64//expansion, 64, 128, 256, 512]
        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],
                                        n_blocks=l, stride=1 if i == 0 else 2)
                            for i, l in enumerate(layers)]
        res = cls(*stem, nn.MaxPool2d(kernel_size=3, stride=2, padding=1), *res_layers,
                        nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(nfs[-1]*expansion, c_out),)
        init_cnn(res)
        return res

    @staticmethod
    def _make_layer(expansion, ni, nf, n_blocks, stride):
        return nn.Sequential(*[ResBlock(expansion, ni if i == 0 else nf, nf, stride if i==0 else 1)
                                    for i in range(n_blocks)])


def xresnet18(**kwargs):     return XResNet.create(1, [2,2,2,2], **kwargs)

def xresnet34(**kwargs):     return XResNet.create(1, [3,4,6,3], **kwargs)

def xresnet50(**kwargs):     return XResNet.create(4, [3,4,6,3], **kwaargs)

def xresnet101(**kwargs):    return XResNet.create(4, [3,4,23,3], **kwargs)

def xresnet512(**kwargs):    return XResNet.create(4, [3,8,36,3], **kwargs)

def get_batch(dl, learn):
    learn.xb, learn.yb = next(iter(dl))
    learn.do_begin_fit(0)
    learn("begin_batch")
    learn("after_fit")
    return learn.xb, learn.yb

def model_summary(model, data, find_all=False, print_mod=False):
    xb, yb = get_batch(data.valid_dl, learn)
    mods = find_modules(model, is_lin_layer) if find_all else model.children()
    f = lambda hook, mod, inp, out: print(f"==\n{mod}\n" if print_mod else "", out.shape)
    with Hooks(mods, f) as hooks:
        learn.model(xb)

def create_phases(phases):
    phases = listify(phases)
    return phases + [1-sum(phases)]


def cnn_learner(arch, data, loss_func, opt_func, c_in=None, c_out=None, lr=1e-2, cuda=True,
                    norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs):
    cbfs = [partial(AvgStatsCallback, accuracy)] + listify(xtra_cb)
    if progress:     cbfs.append(ProgressCallback)
    if cuda:         cbfs.append(CudaCallback)
    if norm:         cbfs.append(partial(BatchTransformXCallback, norm))
    if mixup:        cbfs.append(partial(Mixup, mixup))
    arch_args = {}
    if not c_in:     c_in = data.c_in
    if not c_out:    c_out = data.c_out
    if c_in:         arch_args["c_in"]  = c_in
    if c_out:        arch_args["c_out"] = c_out
    return Learner(arch(**arch_args), data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs)

def random_splitter(fn, p_valid):
    return random.random() < p_valid

from types import SimpleNamespace
cb_type = SimpleNamespace(**{o:o for o in Learner.ALL_CBS})

class DebugCallback(Callback):
    _order = 999
    def __init__(self, cb_name, f=None):
        self.cb_name, self.f = cb_name, f
    def __call__(self, cb_name):
        if cb_name == self.cb_name:
            if self.f:
                self.f(self.run)
            else:
                set_trace()

def sched_1cycle(lrs, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95):
    phases = create_phases(pct_start)
    sched_lr  = [combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))
                 for lr in lrs]
    sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))
    return [ParamScheduler('lr', sched_lr),
            ParamScheduler('mom', sched_mom)]